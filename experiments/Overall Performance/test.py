import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn.glob import global_add_pool
import numpy as np
from datetime import datetime
import random
import networkx as nx
import matplotlib.pyplot as plt
import time

def sample_gumbel(shape, eps=1e-20):
    U = torch.rand(shape)
    return -torch.log(-torch.log(U + eps) + eps)

def gumbel_softmax_sample(logits, temperature=1):
    y = logits + sample_gumbel(logits.shape)
    return F.softmax(y / temperature, dim=-1)

def gumbel_softmax(logits, temperature=1, hard=True):
    """
    ST-gumple-softmax
    input: [*, n_class]
    return: flatten --> [*, n_class] an one-hot vector
    """
    y = gumbel_softmax_sample(logits, temperature)

    if not hard:
        return y

    shape = y.shape
    _, ind = y.max(dim=-1)
    print("ind", ind)
    y_hard = torch.zeros(shape).view(-1, shape[-1])
    y_hard.scatter_(1, ind.view(-1, 1), 1)
    y_hard = y_hard.view(*shape)

    # Set gradients w.r.t. y_hard gradients w.r.t. y
    y_hard = (y_hard - y).detach() + y
    print("y", y)
    print("y_hard", y_hard)
    return y_hard

def gumbel_softmax_test(logits, n, temperature=1):
    y = gumbel_softmax_sample(logits, temperature)
    print(y)

    _, ind = torch.topk(y, n)  # 选出top-n
    y_hard = torch.zeros_like(y)  # 初始化一个与y同形状的全0张量
    y_hard.scatter_(0, ind, 1)  # 根据索引填充

    # 设置关于y_hard和y的梯度
    y_hard = (y_hard - y).detach() + y
    return y_hard

def transform_matrix(matrix):
    n, m = matrix.shape
    
    # 进行n次操作
    for _ in range(n):
        # 找到最大元素的索引
        max_index = np.unravel_index(np.argmax(matrix, axis=None), matrix.shape)
        max_row, max_col = max_index
        
        # 设置最大元素所在行列的值为0，最大元素自身设置为1
        matrix[max_row, :] = 0
        matrix[:, max_col] = 0
        matrix[max_row, max_col] = 1
    
    return matrix

def apply_gumbel_softmax():
    logits1 = torch.tensor([[0.1], [0.2], [0.3], [0.4], [0.5], [0.6], [0.7]], dtype=torch.float32)
    logits2 = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], dtype=torch.float32)
    logits3 = torch.tensor([0.11, 0.22, 0.33, 0.44, 0.55], dtype=torch.float32)

    print(torch.cat((logits2.repeat(3, 1), logits3.repeat(3, 1)), dim=1))

    print(logits1.repeat(3, 1))
    print(logits1.size())
    print(logits1.squeeze())
    print(logits1.squeeze(-1).size())
    for i in range(5):
        print(gumbel_softmax_test(logits1, 5))
        print("---------------------")

    for i in range(5):
        print(gumbel_softmax(logits1))
        print("---------------------")

# --------------------------------------------------------------------------------------

def mask_operation():
    # 示例：创建一个n*m的张量x
    n, m = 5, 4  # 假设张量x的形状为5x4
    x = torch.arange(n*m).view(n, m)  # 创建一个示例张量
    print("原始张量x:\n", x)
    rows, cols = x.shape[:2]
    print(rows, cols)
    print(x.numel())
    print(x.view(1, x.numel()))
    # 索引张量，形状为n*1，这里用一个示例，表示要归零第2和第4行（索引从0开始）
    index = torch.tensor([0, 1, 0, 1])
    print(index)
    # 将索引张量转换为布尔张量
    mask = index == 1
    mask = mask.expand_as(x)
    print(mask)
    # 使用布尔索引归零对应的行
    print("\n修改后的张量x:\n", x[mask])
    x[mask.expand_as(x)] = 0
    print("\n修改后的张量x:\n", x)

#---------------------------------------------------------------------------------------
def BCELoss():
    p = torch.tensor([[0, 1, 0],
                    [1, 0, 0],
                    [0, 0, -1]], dtype=torch.float32)
    t = torch.tensor([[1, 0, 0],
                    [0, 1, 0],
                    [0, 0, 1]], dtype=torch.float32)
    loss = torch.nn.BCEWithLogitsLoss()
    print(loss(p, t))


# -------------------------------------------------------------------------------------------
def get_date():
    # 获取当前日期和时间
    now = '_'+datetime.now().strftime('%y-%m%d-%H%M')+'.txt'
    print(now)

# -------------------------------------------------------------------------------------------

def log_sinkhorn_norm(log_alpha: torch.Tensor, n_iter: int =20):
    for _ in range(n_iter):
        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)
        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)
    return log_alpha.exp()

def gumbel_sinkhorn(log_alpha: torch.Tensor, tau: float = 1.0, n_iter: int = 20, noise: bool = True):
    if noise:
        uniform_noise = torch.rand_like(log_alpha)
        gumbel_noise = -torch.log(-torch.log(uniform_noise+1e-20)+1e-20)
        log_alpha = (log_alpha + gumbel_noise)/tau
    sampled_perm_mat = log_sinkhorn_norm(log_alpha, n_iter)
    return sampled_perm_mat

def apply_gs():
    p = torch.tensor([[0.1, 0.9, 0.3, 0.2, 0.0],
                        [0.4, 0.5, 0.6, 0.4, 0.0],
                        [0.4, 0.3, 0.7, 0.1, 0.0],
                        [0.0, 0.0, 0.0, 0.0, 0.0],
                        [0.0, 0.0, 0.0, 0.0, 0.0]], dtype=torch.float32)
    r = gumbel_sinkhorn(p, 0.1)
    row_sums = torch.sum(r, dim=1)
    col_sums = torch.sum(r, dim=0)
    print(row_sums)
    print(col_sums)
    print(r)

# ------------------------------------------------------------------------
def pad_tensor():
    p = torch.tensor([[0.1, 0.9, 0.3],
                        [0.4, 0.5, 0.6],
                        [0.4, 0.3, 0.7]], dtype=torch.float32)

    r = F.pad(p,pad=(0,1,0,2)) # 左右上下
    print(r)

# ------------------------------------------------------------------------
def generate_line_graph():
    edge_index = [[7, 3], [3, 1], [3, 8], [3, 9], [5, 2], [6, 2], [1, 0], [0, 2], [4, 2]]
    features = [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
    features = torch.tensor(features).float()
    print(features.shape)
    G = nx.Graph()
    # 生成线图边索引
    edge_index = [tuple(i) for i in edge_index]    # 元素转换为元组
    G.add_edges_from(edge_index)
    L = nx.line_graph(G)  # 生成线图L(G)
    edge_index_2 = L.edges()  # 得到线图的边索引
    # 生成线图节点特征向量
    feature_list = []
    for node in L.nodes():
        f = features[node[0]] + features[node[1]]
        feature_list.append(f.unsqueeze(0))
    features_2 = torch.cat(feature_list, dim=0)  
    print(features_2.shape)
    node_mapping = {edge: idx for idx, edge in enumerate(L.nodes())}
    L_mapped = nx.Graph()
    for edge in L.edges():
        # 获取映射后的节点索引
        new_u = node_mapping[edge[0]]
        new_v = node_mapping[edge[1]]
        # 在新图中添加边
        L_mapped.add_edge(new_u, new_v)
    print("原边图的节点:", list(L.nodes()))
    print("映射后的节点:", node_mapping)
    print("原边图的边:", L.edges())
    print("新边图的边:", L_mapped.edges())

    # 绘制原图G
    plt.figure(figsize=(12, 6))
    plt.subplot(121)
    nx.draw(G, with_labels=True, node_color='orange', edge_color='black', node_size=1000, font_size=16)
    plt.title('Original Graph G')
    # 绘制线图L(G)
    plt.subplot(122)
    nx.draw(L, with_labels=True, node_color='lightblue', edge_color='black', node_size=1000, font_size=16)
    plt.title('Line Graph L(G)')
    plt.show()

def transform_to_line_graph():
    edge_index = torch.tensor([
        [7, 7, 3, 5, 8, 6, 1, 1, 0, 6, 9, 0, 2, 6, 4, 0, 4, 2, 0, 1, 2, 3, 4, 5,
         6, 7, 8, 9],
        [6, 9, 0, 2, 6, 4, 0, 4, 2, 7, 7, 3, 5, 8, 6, 1, 1, 0, 0, 1, 2, 3, 4, 5,
         6, 7, 8, 9]])
    # 去除原图边索引中的自环和反向对
    _, n = edge_index.shape
    x = edge_index[0, -1].item()
    edge_index = edge_index[:, :n-x-1]
    _, n = edge_index.shape
    edge_index = edge_index[:, :int(n/2)]
    print(edge_index)

def node_alignment_with_edge():
    map_matrix = torch.tensor([
        [-0.0876, -0.0401, -0.0679, -0.0539,  0.0392, -0.2828, -0.2195, -0.0533, -0.1780, -0.0579],
        [-0.0534, -0.1842, -0.1351, -0.1889, -0.1086, -0.2122, -0.1998, -0.1768, -0.1016, -0.1773],
        [-0.1114, -0.1309, -0.1408, -0.0850, -0.0772, -0.2847, -0.2259, -0.2473, -0.2625, -0.2388],
        [-0.1479, -0.1357, -0.2550, -0.1085, -0.2229, -0.1593, -0.2189, -0.0714, -0.2016, -0.1402],
        [-0.1510, -0.1799, -0.1988, -0.2105, -0.1961, -0.1336, -0.2262, -0.2012, -0.1765, -0.2066],
        [-0.1833, -0.2587, -0.1745, -0.1439, -0.1256, -0.1921, -0.2084, -0.0687, -0.2650, -0.1643],
        [-0.1831, -0.2106, -0.1560, -0.2474, -0.2259, -0.1658, -0.1147, -0.1671, -0.1263, -0.1683],
        [-0.2363, -0.2498, -0.0784, -0.2131, -0.1367, -0.1598, -0.1704, -0.1169, -0.1856, -0.1105],
        [-0.1901, -0.3069,  0.0068, -0.3052, -0.2435, -0.0969, -0.0535, -0.1415, -0.1045, -0.1632],
        [-0.2426, -0.3135,  0.0143, -0.2249, -0.0932, -0.1302, -0.1505, -0.1765, -0.1094, -0.1907]])
    lg_map_matrix = torch.tensor([
        [-0.0174, -0.0569,  0.0153, -0.0872, -0.0328,  0.0371,  0.0961,  0.0277,  0.0971,  0.0274],
        [ 0.0112, -0.0126,  0.0204,  0.1814,  0.1416,  0.0743,  0.1157, -0.0636, -0.0100, -0.0223],
        [-0.0380,  0.0420,  0.0623,  0.0448, -0.0204, -0.0494, -0.0158,  0.0099,  0.0199,  0.0354],
        [ 0.0390, -0.0507, -0.0554,  0.0489, -0.0484, -0.0530,  0.0103, -0.0065, -0.0228, -0.0496],
        [ 0.0981,  0.0433, -0.0742,  0.0914, -0.0243,  0.0038, -0.0543, -0.0250, -0.0089, -0.0010],
        [-0.0235, -0.0165,  0.0691, -0.0567, -0.0114,  0.0080, -0.0235, -0.0239, -0.0155, -0.0181],
        [-0.0289,  0.0035, -0.0166, -0.0152,  0.0134, -0.0119,  0.0633,  0.0041,  0.0342,  0.0003],
        [-0.0104,  0.0826,  0.0475,  0.2230, -0.0193,  0.0216,  0.1876,  0.0101,  0.1135, -0.0113],
        [-0.0288, -0.0163, -0.0098,  0.0707,  0.1436, -0.0418,  0.2567,  0.0139, -0.0300,  0.0750]])
    lg_node_list_1 = [(7, 9), (3, 0), (5, 2), (6, 8), (0, 2), (6, 4), (7, 6), (0, 1), (4, 1)]
    lg_node_list_2 = [(3, 1), (2, 4), (2, 6), (1, 0), (2, 0), (5, 2), (7, 5), (7, 9), (9, 8), (8, 6)]
    print("节点相似度矩阵:", map_matrix.shape)
    print("边相似度矩阵:", lg_map_matrix.shape)
    print("-------------------------------")
    # print(len(lg_node_list_1))
    # print(len(lg_node_list_2))
    aligment_index = []
    n, m = map_matrix.shape
    x = min(n, m)  # 最多对齐的节点数
    i = 0
    while i < x:
        """ 1、从节点相似度矩阵中得到节点对x """
        max_index = torch.argmax(map_matrix).item()
        max_row = max_index // m
        max_col = max_index % m
        print("原图节点对x:", max_row, max_col)
        aligment_index.append([max_row, max_col])
        i += 1
        if i == x: break
        map_matrix[max_row, :] = -1
        map_matrix[:, max_col] = -1
        """ 2、根据节点对x, 从边相似度矩阵找到另一节点对y """
        row_list = []  # 在边相似度矩阵中，与得到“原图节点”索引相关的“边图节点”索引
        col_list = []
        for index in range(len(lg_node_list_1)):
            if max_row in lg_node_list_1[index]: row_list.append(index)
        for index in range(len(lg_node_list_2)):
            if max_col in lg_node_list_2[index]: col_list.append(index)
        # print("row_list:", row_list)
        # print("col_list:", col_list)
        # 得到边图节点对z
        max_score = -1
        lg_node_1, lg_node_2 = (), ()
        for j in row_list:  # 遍历边相似度度矩阵中“原图节点”相关的元素，选择最大值
            for k in col_list:
                if lg_map_matrix[j][k] > max_score:
                    max_score = lg_map_matrix[j][k]  # 更新最大值
                    lg_node_1 = lg_node_list_1[j]  # 更新索引
                    lg_node_2 = lg_node_list_2[k]
        print("边图节点对z:", lg_node_1, lg_node_2)
        # 从边图节点对z中得到原图节点对y
        new_row = lg_node_1[0] if max_row == lg_node_1[1] else lg_node_1[1]  
        new_col = lg_node_2[0] if max_col == lg_node_2[1] else lg_node_2[1]
        print("原图节点对y:", new_row, new_col)
        """ 3、根据节点相似度矩阵的情况, 判断节点对y是否舍弃 """
        if map_matrix[new_row, new_col] != -1:
            aligment_index.append([new_row, new_col])
            i += 1
            map_matrix[new_row, :] = -1
            map_matrix[:, new_col] = -1
            print("1")
        print("-----------------------")
        
    map_matrix.zero_()
    for index in aligment_index:
        map_matrix[index[0], index[1]] = 1
    print(map_matrix)
    return map_matrix

def Euclidean_Distance():
    # 假设 embeddings1 是 n x 32 的张量，embeddings2 是 m x 32 的张量
    n, m, d = 2, 3, 32  # 例如 n = 5
    embeddings1 = torch.randn(n, d)
    embeddings2 = torch.randn(m, d)
    print(embeddings1)
    print(embeddings2)

    # 扩展两个张量以进行向量化距离计算
    # embeddings1.unsqueeze(1) 将变成 n x 1 x 32
    # embeddings2.unsqueeze(0) 将变成 1 x m x 32
    # 结果将会广播成 n x m x 32
    diff = embeddings1.unsqueeze(1) - embeddings2.unsqueeze(0)

    # 计算欧氏距离：先平方，然后在最后一个维度上求和，最后开根号
    dist_matrix = torch.sqrt(torch.sum(diff ** 2, dim=2))  # n*m
    print(dist_matrix)
    # 计算最小值和最大值
    min_val = torch.min(dist_matrix)
    max_val = torch.max(dist_matrix)
    # 进行最小-最大归一化
    normalized_dist_matrix = (dist_matrix - min_val) / (max_val - min_val)

    print("Cost matrix (n x m):\n", normalized_dist_matrix)

def f1():
    """ 节点相似度矩阵和边相似度矩阵得到一个有偏向性的噪声代替gs中的随机噪声"""
    map_matrix = torch.tensor([
        [-0.0876, -0.0401, -0.0679, -0.0539,  0.0392, -0.2828, -0.2195, -0.0533, -0.1780, -0.0579],
        [-0.0534, -0.1842, -0.1351, -0.1889, -0.1086, -0.2122, -0.1998, -0.1768, -0.1016, -0.1773],
        [-0.1114, -0.1309, -0.1408, -0.0850, -0.0772, -0.2847, -0.2259, -0.2473, -0.2625, -0.2388],
        [-0.1479, -0.1357, -0.2550, -0.1085, -0.2229, -0.1593, -0.2189, -0.0714, -0.2016, -0.1402],
        [-0.1510, -0.1799, -0.1988, -0.2105, -0.1961, -0.1336, -0.2262, -0.2012, -0.1765, -0.2066],
        [-0.1833, -0.2587, -0.1745, -0.1439, -0.1256, -0.1921, -0.2084, -0.0687, -0.2650, -0.1643],
        [-0.1831, -0.2106, -0.1560, -0.2474, -0.2259, -0.1658, -0.1147, -0.1671, -0.1263, -0.1683],
        [-0.2363, -0.2498, -0.0784, -0.2131, -0.1367, -0.1598, -0.1704, -0.1169, -0.1856, -0.1105],
        [-0.1901, -0.3069,  0.0068, -0.3052, -0.2435, -0.0969, -0.0535, -0.1415, -0.1045, -0.1632],
        [-0.2426, -0.3135,  0.0143, -0.2249, -0.0932, -0.1302, -0.1505, -0.1765, -0.1094, -0.1907]])
    aligment_matrix = node_alignment_with_edge()
    print("++++++++++++++++++++++++++++++")
    # print("uniform_noise:\n", uniform_noise)
    for i in range(10):
        uniform_noise = torch.rand_like(map_matrix)
        gumbel_noise = -torch.log(-torch.log(uniform_noise+1e-20)+1e-20)
        # print("gumbel_noise:\n", gumbel_noise)
        # biased_gumbel_noise = torch.mul(gumbel_noise, (1 + aligment_matrix * 0.5))
        biased_gumbel_noise = gumbel_noise + torch.mul(aligment_matrix, map_matrix)
        # print("biased_gumbel_noise:\n", biased_gumbel_noise)
        a1 = (map_matrix + gumbel_noise)/0.1
        a2 = (map_matrix + biased_gumbel_noise)/0.1
        result1 = log_sinkhorn_norm(a1, 20)
        result2 = log_sinkhorn_norm(a2, 20)
        # print("row_sums:", torch.sum(result1, dim=1))
        # print("col_sums:", torch.sum(result1, dim=0))
        # print("result:\n", result)
        print(torch.sum(torch.mul(aligment_matrix, result1)))
        # print("row_sums:", torch.sum(result2, dim=1))
        # print("col_sums:", torch.sum(result2, dim=0))
        print(torch.sum(torch.mul(aligment_matrix, result2)))
        print("--------------------------------")

def generate_pseudo_graph():
    LRL_map_matrix = torch.tensor([[ 3.6558,  2.6812,  3.6288,  2.3134,  2.8225,  1.3023,  2.9987,  0.7263,
          2.3455,  1.2070],
        [ 3.1277,  2.4902,  1.3312,  2.5804,  2.5862,  0.9125, -0.2894,  1.0546,
          0.3716,  1.4656],
        [ 2.7783,  2.4167,  1.0903,  2.4318,  1.3006,  0.9251,  1.3552,  0.5571,
          1.4080,  0.9650],
        [ 1.3930,  1.0067,  4.2007,  0.8409,  2.2024,  1.7265,  5.4032,  1.4084,
          3.7964,  1.6337],
        [ 2.1642,  1.5178,  0.8079,  1.8438,  1.6571,  0.8653, -0.1806,  0.9541,
          0.3604,  1.3684],
        [ 3.0409,  2.9953,  0.9685,  3.4027,  1.3385,  1.6998,  1.1102,  1.3566,
          1.1456,  1.3957],
        [ 1.5046,  1.5956,  2.3636,  1.8149,  0.2339,  1.6721,  2.9927,  1.9996,
          2.5261,  2.5288],
        [ 0.6665,  1.0857,  2.3604,  1.4508, -0.0178,  2.1884,  3.6495,  2.1523,
          2.6902,  2.1039],
        [ 2.1203,  1.8209,  1.4331,  2.1061,  1.1942,  1.4903,  0.1692,  1.2159,
          0.7797,  1.6434],
        [ 0.5317,  0.6179,  1.5566,  1.3774,  0.9254,  1.7263,  0.6545,  1.6711,
          0.8586,  1.9258]])
    node_alignment = torch.tensor([[7.6191e-01, 4.3638e-04, 4.6237e-04, 5.0227e-21, 1.2856e-20, 2.9709e-07,
         2.1836e-01, 4.7920e-18, 8.8774e-22, 4.5777e-28],
        [2.3784e-01, 7.8190e-01, 2.7456e-10, 5.1071e-06, 9.4339e-07, 5.7323e-08,
         1.6601e-17, 1.2845e-06, 2.9124e-08, 3.9063e-11],
        [1.6215e-14, 5.6947e-02, 7.0070e-24, 9.8166e-01, 8.1177e-16, 5.0194e-12,
         5.9927e-21, 1.9514e-26, 1.3898e-22, 3.4516e-04],
        [1.2631e-15, 1.2543e-06, 9.9954e-01, 3.3484e-17, 4.1470e-07, 9.9073e-12,
         7.9004e-04, 3.2753e-15, 3.9180e-09, 4.9515e-18],
        [8.8758e-22, 7.5751e-02, 1.0329e-28, 3.0455e-17, 1.0000e+00, 1.9477e-08,
         9.6756e-34, 2.0304e-12, 7.8432e-26, 5.0443e-22],
        [3.8685e-11, 3.7068e-02, 3.3669e-21, 1.5791e-20, 1.2676e-18, 2.4314e-17,
         3.9771e-23, 9.3364e-01, 8.8046e-02, 3.6225e-26],
        [5.1707e-08, 4.7876e-02, 7.2644e-11, 2.8558e-06, 2.4524e-07, 4.4597e-04,
         1.1502e-03, 6.6357e-02, 9.1195e-01, 1.5729e-02],
        [2.3670e-30, 4.9278e-10, 1.1745e-12, 1.5014e-20, 4.0086e-23, 1.6719e-01,
         7.7970e-01, 1.6201e-11, 1.3650e-15, 4.8167e-17],
        [2.5269e-04, 2.4074e-05, 1.7200e-10, 1.8333e-02, 3.4770e-08, 8.3236e-01,
         1.7399e-14, 2.1286e-08, 7.7456e-08, 7.4228e-02],
        [1.1937e-30, 5.0344e-13, 1.3604e-24, 6.8131e-29, 1.0815e-24, 9.1142e-11,
         2.5022e-28, 2.3125e-22, 1.6106e-23, 9.0970e-01]])
    features_1 = torch.tensor([
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
    features_2 = torch.tensor([
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
    
    LRL_map_matrix2 = torch.tensor([[2.7499, 3.3869, 2.1436, 0.5461, 2.6860, 0.3504, 2.6139, 0.6617, 1.7120],
        [1.7690, 1.5670, 1.8065, 2.3701, 3.3928, 2.6389, 0.5856, 2.1788, 0.3836],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])
    node_alignment2 = torch.tensor([[1.0222e-01, 9.9068e-22, 2.1861e-22, 5.2848e-33, 2.9319e-08, 1.8642e-31,
         9.9967e-01, 9.7606e-29, 1.5979e-22],
        [1.2361e-08, 7.1230e-09, 7.8527e-17, 1.1293e-04, 3.3957e-07, 9.9998e-01,
         1.5836e-15, 1.3064e-02, 1.2233e-13],
        [1.2413e-07, 4.5769e-06, 1.5572e-09, 5.5760e-06, 9.9997e-01, 3.4584e-08,
         8.8599e-06, 3.5034e-04, 7.4408e-07],
        [1.1482e-04, 1.2116e-08, 7.1358e-11, 1.3096e-08, 2.3492e-08, 2.0558e-09,
         2.5723e-04, 9.8552e-01, 2.7049e-14],
        [8.9766e-01, 3.2179e-08, 5.7253e-02, 5.9148e-02, 3.3612e-05, 1.2829e-07,
         5.5771e-05, 1.0690e-03, 1.2485e-09],
        [1.3181e-16, 1.0000e+00, 7.5302e-12, 2.3822e-19, 5.0217e-10, 2.2081e-05,
         8.2424e-09, 2.6906e-16, 2.9796e-21],
        [6.0131e-15, 1.9150e-22, 9.4275e-01, 5.9377e-15, 5.2010e-08, 6.2558e-15,
         9.8921e-09, 1.8274e-23, 9.4437e-17],
        [2.4744e-08, 2.6715e-23, 1.9205e-20, 9.4073e-01, 3.3243e-26, 3.1763e-13,
         8.4846e-23, 1.6258e-25, 1.1271e-16],
        [3.8773e-17, 8.7064e-16, 2.2256e-15, 9.2274e-07, 1.1798e-17, 3.3223e-07,
         7.9351e-06, 8.8787e-18, 1.0000e+00]])
    features_12 = torch.tensor([
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
    features_22 = torch.tensor([
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
    n1 = features_1.shape[0]
    # 转化为0-1矩阵
    binarized_matrix = (node_alignment >= 0.5).float()
    # 裁剪成n1*n2大小
    cropped_matrix = binarized_matrix[:n1, :]  # 数据集中图1应该都是小于图2的
    # 获取所有元素为 1 的索引tensor([[x,y],[x,y],...])
    indices = torch.nonzero(cropped_matrix)  
    value = torch.tensor([LRL_map_matrix[i[0], i[1]] for i in indices])  # 索引在相似度矩阵的对应的元素
    k = int(len(value) * 0.6)
    if k < 1: print("error")
    # 获取前k个最大元素的值及其原索引
    _, value_topk_indices = torch.topk(value, k)  # tensor([,,])
    # 交换特征
    pseudo_features_1 = features_1.clone()
    pseudo_features_2 = features_2.clone()
    for x in value_topk_indices:
        indice = indices[x]  # 通过value索引获得对应的索引
        i = indice[0]
        j = indice[1]
        pseudo_features_1[i] = features_2[j]
        pseudo_features_2[j] = features_1[i]
    print("indices =", indices)
    print("valve =", value)
    print("k =", k)
    print("value_topk_indices =", value_topk_indices)
    print("---------------------------------")
    print(LRL_map_matrix.shape)
    print(features_1.shape)
    print(features_2.shape)
    print("---------------------------------")
    print(pseudo_features_1)
    print(pseudo_features_2)
    
def AReg():
    N1 = torch.tensor([[0.5, 0.3, 0.1], [0.7, 0.8, 0.9]], dtype=torch.float32)
    G1 = torch.tensor([0.4, 0.6, 0.8], dtype=torch.float32).unsqueeze(0)
    G2 = torch.tensor([0.1, 0.3, 0.5], dtype=torch.float32).unsqueeze(0)
    print(torch.t(G1).shape)
    # 将G1和G2重复N1的第一维度，以匹配N1的形状
    G1 = G1.repeat(N1.size(0), 1)
    G2 = G2.repeat(N1.size(0), 1)
    # 计算欧式距离
    D_N1_G1 = torch.norm(N1 - G1, dim=1)
    D_N1_G2 = torch.norm(N1 - G2, dim=1)
    print("D_N1_G1:", D_N1_G1)
    print("D_N1_G2:", D_N1_G2)
    loss = F.relu(D_N1_G1 - D_N1_G2 + 1.0)
    print(loss)
    loss = loss.mean() + loss.mean()
    print("损失值 L1:", loss.item())
        

if __name__ == "__main__":
    # node_alignment_with_edge()
    # transform_to_line_graph()
    # ----------------------------------
    # f1()
    # node_alignment_with_edge()
    # generate_pseudo_graph()
    
    # 3*9
    # m1 = torch.tensor([[0.2390, -0.2890, 0.0310, -0.1054, -0.0715, 0.1375, 0.0272, -0.0623, 0.0371],
    #     [0.0274, 0.0454, -0.4693, -0.0278, 0.0542, 0.1346, 0.1167, 0.0742, -0.0504],
    #     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])
    # m2 = torch.tensor([[0.1137, 0.0964, 0.1158, 0.1158, 0.1035, 0.0911, 0.0993, 0.1227, 0.1417],
    #     [0.1137, 0.0964, 0.1158, 0.1158, 0.1035, 0.0911, 0.0993, 0.1227, 0.1417],
    #     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])
    # # 2*9
    # m3 = torch.tensor([[0.2390, 0.2890, 0.0310, 0.1054, 0.0715, 0.1375, 0.0272, 0.0623, 0.0371],
    #     [0.0274, 0.0454, 0.4693, 0.0278, 0.0542, 0.1346, 0.1167, 0.0742, 0.0504]])
    # m4 = torch.tensor([[0.1137, 0.0964, 0.1158, 0.1158, 0.1035, 0.0911, 0.0993, 0.1227, 0.1417],
    #     [0.1137, 0.0964, 0.1158, 0.1158, 0.1035, 0.0911, 0.0993, 0.1227, 0.1417]])
    
    # m5 = torch.tensor([[0.2390, -0.2890, 0.0310, -0.1054, -0.0715, 0.1375, 0.0272, -0.0623, 0.0371],
    #     [0.0274, 0.0454, -0.4693, -0.0278, 0.0542, 0.1346, 0.1167, 0.0742, -0.0504]])
    # m6 = torch.tensor([[-0.1137, -0.0964, 0.1158, -0.1158, 0.1035, -0.0911, 0.0993, 0.1227, 0.1417],
    #     [0.1137, -0.0964, 0.1158, 0.1158, -0.1035, 0.0911, -0.0993, 0.1227, 0.1417]])
    
    # m = torch.nn.Softmax(dim=1)
    # loss1 = F.cross_entropy(m1, m2, reduction='mean')
    # loss2 = F.cross_entropy(m(m1), m(m2), reduction='mean')
    
    # loss3 = F.cross_entropy(m3, m4, reduction='mean')
    # loss8 = F.cross_entropy(m3, m4, reduction='sum')
    # loss4 = F.cross_entropy(m(m3), m(m4), reduction='mean')
    # loss7 = F.cross_entropy(m(m3), m(m4), reduction='sum')
    
    # loss5 = F.cross_entropy(m5, m6, reduction='mean')
    # loss6 = F.cross_entropy(m(m5), m(m6), reduction='mean')
    # print(loss1)
    # print(loss2)
    # print("---------------")
    # print(loss3)
    # print(loss8)
    # print(loss4)
    # print(loss7)
    # print("---------------")
    # print(loss5)
    # print(loss6)
    
    # m1 = torch.tensor([[0.2, -0.2]])
    # m2 = torch.tensor([[0.1, 0.1]])
    # loss = F.mse_loss(m1, m2)
    # print(loss)
    
    # AReg()
    # Euclidean_Distance()
    # a = torch.tensor([
    #     [-1.4104, -0.6720,  0.1438, -1.3868, -0.0204, -0.1402, -0.8980,  0.7575,
    #      -0.4488,  2.3645, -1.1577, -1.0400,  1.2482,  0.5957,  0.9102,  0.3775,
    #       0.5884, -1.7018, -0.0324,  0.2365, -0.2681, -0.8941, -0.4933, -1.2182,
    #      -0.9712, -1.4654, -0.6935, -1.6483, -1.3283,  1.1150,  0.3481, -0.7960],
    #     [ 0.6438,  0.7655, -1.3322,  0.2552, -0.6008, -1.9216,  0.7924,  0.2994,
    #       0.1868, -0.3612, -0.6188,  0.3797, -1.8240,  1.1564,  0.9698, -0.7456,
    #       0.5676, -0.7280,  1.8041,  1.0853, -1.3753, -0.6080, -1.4129, -0.9800,
    #       0.5361, -1.4069,  0.3486, -0.3882, -1.5718,  0.9543,  0.9196, -0.9867],
    #     [ 1.1776,  0.3524,  0.7700,  0.1023,  1.2658,  1.5680, -1.8943,  0.9061,
    #       2.0382, -0.0066, -1.1558, -0.3182,  1.1873, -1.3283,  0.5061,  1.6650,
    #       1.3099,  0.3044, -1.5438, -1.3673,  1.6536,  0.8686,  1.1761,  0.0720,
    #       0.5687,  0.6782, -1.8357, -0.8842, -0.3725,  1.4147,  1.0399, -0.2283],
    #     [-0.6114, -1.5201,  1.1875,  0.4768, -1.1851, -0.2848,  0.7789, -0.9366,
    #      -1.2862, -0.1076,  0.5100, -1.3435, -0.4062,  1.2048,  0.1262,  0.1602,
    #      -0.7616,  0.6158,  1.0510,  0.5706, -0.0751, -0.7503,  0.6458,  0.4532,
    #       0.1242,  0.2184,  0.8836,  0.8871,  0.2160, -1.4021, -0.0761,  2.2115],
    #     [-0.5067, -1.4028, -0.7493, -1.9276,  0.8843, -0.5305,  0.2293,  0.3159,
    #      -1.1570,  0.4795,  1.1773,  1.7522,  0.5191,  0.7038, -0.2853, -0.8370,
    #      -0.0996, -1.1477, -1.0703,  0.0482, -0.4964,  2.0074, -0.0579, -1.2708,
    #      -2.2071,  0.1127,  1.3005, -0.2324,  0.2101, -0.8185, -2.2531, -0.9097],
    #     [-0.4698,  1.1929, -1.5591,  0.7891, -0.9185, -0.2777,  0.8595,  0.1942,
    #      -0.1360, -0.7504,  0.7079,  1.1636, -0.8697, -0.1194, -0.5965, -1.1337,
    #      -0.9859,  0.3939,  0.1585,  0.7926, -0.8813, -0.0931, -0.9958,  0.6863,
    #       0.2116, -0.4926,  0.4132,  0.9161,  1.1951, -0.9254, -0.3250,  0.7347],
    #     [ 1.7423,  0.2485,  0.8757,  0.8882,  1.4792,  1.0434, -0.8199,  0.7012,
    #       0.7714, -1.0955, -0.8823, -0.6609,  0.5741, -1.3647,  0.6325,  1.3490,
    #       1.0801,  0.9475, -0.0918, -1.9090,  1.5061,  0.5513,  1.6453,  0.5331,
    #       0.8225,  1.5872, -1.0132, -0.2629,  0.2163,  0.2106,  0.7656,  0.1880],
    #     [-0.5654,  1.0356,  0.6636,  0.8028, -0.9044,  0.5434,  0.9520, -2.2378,
    #       0.0315, -0.5227,  1.4194,  0.0672, -0.4288, -0.8483, -2.2630, -0.8353,
    #      -1.6989,  1.3159, -0.2751,  0.5431, -0.0633, -1.0818, -0.5073,  1.7246,
    #       0.9152,  0.7684,  0.5965,  1.6126,  1.4352, -0.5486, -0.4190, -0.2135]])
    # col_sums = torch.sum(a, dim=0)
    # col_means = col_sums / a.size(0)
    # G1 = col_means.unsqueeze(0)
    # print(col_sums)
    # print(col_means)
    # print(G1)
    # 创建一个批归一化层，输入通道数为 64
    batch_norm = nn.BatchNorm1d(num_features=4)

    # 创建一个示例输入张量，形状为 (batch_size, num_features)
    x = torch.randn(10, 4)
    output = batch_norm(x)
    output = torch.nn.functional.relu(output)
    
    print(output)
    # batch = torch.tensor([0,0,1,1,1,1,1,2,2,2], dtype=torch.int64)
    batch = torch.zeros(x.size(0), dtype=torch.int64)

    g_1 = global_add_pool(output, batch)  

    print(g_1)


# tensor([[-1.4104, -0.6720,  0.1438, -1.3868, -0.0204, -0.1402, -0.8980,  0.7575,
#          -0.4488,  2.3645, -1.1577, -1.0400,  1.2482,  0.5957,  0.9102,  0.3775,
#           0.5884, -1.7018, -0.0324,  0.2365, -0.2681, -0.8941, -0.4933, -1.2182,
#          -0.9712, -1.4654, -0.6935, -1.6483, -1.3283,  1.1150,  0.3481, -0.7960],
#         [ 0.6438,  0.7655, -1.3322,  0.2552, -0.6008, -1.9216,  0.7924,  0.2994,
#           0.1868, -0.3612, -0.6188,  0.3797, -1.8240,  1.1564,  0.9698, -0.7456,
#           0.5676, -0.7280,  1.8041,  1.0853, -1.3753, -0.6080, -1.4129, -0.9800,
#           0.5361, -1.4069,  0.3486, -0.3882, -1.5718,  0.9543,  0.9196, -0.9867],
#         [ 1.1776,  0.3524,  0.7700,  0.1023,  1.2658,  1.5680, -1.8943,  0.9061,
#           2.0382, -0.0066, -1.1558, -0.3182,  1.1873, -1.3283,  0.5061,  1.6650,
#           1.3099,  0.3044, -1.5438, -1.3673,  1.6536,  0.8686,  1.1761,  0.0720,
#           0.5687,  0.6782, -1.8357, -0.8842, -0.3725,  1.4147,  1.0399, -0.2283],
#         [-0.6114, -1.5201,  1.1875,  0.4768, -1.1851, -0.2848,  0.7789, -0.9366,
#          -1.2862, -0.1076,  0.5100, -1.3435, -0.4062,  1.2048,  0.1262,  0.1602,
#          -0.7616,  0.6158,  1.0510,  0.5706, -0.0751, -0.7503,  0.6458,  0.4532,
#           0.1242,  0.2184,  0.8836,  0.8871,  0.2160, -1.4021, -0.0761,  2.2115],
#         [-0.5067, -1.4028, -0.7493, -1.9276,  0.8843, -0.5305,  0.2293,  0.3159,
#          -1.1570,  0.4795,  1.1773,  1.7522,  0.5191,  0.7038, -0.2853, -0.8370,
#          -0.0996, -1.1477, -1.0703,  0.0482, -0.4964,  2.0074, -0.0579, -1.2708,
#          -2.2071,  0.1127,  1.3005, -0.2324,  0.2101, -0.8185, -2.2531, -0.9097],
#         [-0.4698,  1.1929, -1.5591,  0.7891, -0.9185, -0.2777,  0.8595,  0.1942,
#          -0.1360, -0.7504,  0.7079,  1.1636, -0.8697, -0.1194, -0.5965, -1.1337,
#          -0.9859,  0.3939,  0.1585,  0.7926, -0.8813, -0.0931, -0.9958,  0.6863,
#           0.2116, -0.4926,  0.4132,  0.9161,  1.1951, -0.9254, -0.3250,  0.7347],
#         [ 1.7423,  0.2485,  0.8757,  0.8882,  1.4792,  1.0434, -0.8199,  0.7012,
#           0.7714, -1.0955, -0.8823, -0.6609,  0.5741, -1.3647,  0.6325,  1.3490,
#           1.0801,  0.9475, -0.0918, -1.9090,  1.5061,  0.5513,  1.6453,  0.5331,
#           0.8225,  1.5872, -1.0132, -0.2629,  0.2163,  0.2106,  0.7656,  0.1880],
#         [-0.5654,  1.0356,  0.6636,  0.8028, -0.9044,  0.5434,  0.9520, -2.2378,
#           0.0315, -0.5227,  1.4194,  0.0672, -0.4288, -0.8483, -2.2630, -0.8353,
#          -1.6989,  1.3159, -0.2751,  0.5431, -0.0633, -1.0818, -0.5073,  1.7246,
#           0.9152,  0.7684,  0.5965,  1.6126,  1.4352, -0.5486, -0.4190, -0.2135]],
#        grad_fn=<NativeBatchNormBackward0>)
# tensor([[ 5.9605e-08,  2.9802e-08, -2.9802e-08,  0.0000e+00, -4.4703e-08,
#          -1.0431e-07, -8.9407e-08,  1.1921e-07, -4.4703e-08,  2.9802e-08,
#           0.0000e+00, -2.9802e-08,  2.2352e-08,  2.2352e-08,  2.9802e-08,
#          -1.4901e-08, -4.4703e-08,  0.0000e+00,  5.2154e-08,  2.2352e-08,
#          -2.9802e-08,  0.0000e+00,  4.4703e-08,  5.9605e-08,  2.2352e-08,
#           5.9605e-08,  2.2352e-08,  1.4901e-08,  4.4703e-08,  1.4901e-08,
#          -2.9802e-08,  2.2352e-08]], grad_fn=<UnsqueezeBackward0>)
# tensor([[ 2.1191,  0.4182,  0.3257, -1.8862,  0.3703, -0.7100,  0.0823, -1.0388,
#          -0.6888, -2.3832, -1.8254, -2.2242, -2.2910,  2.0203,  0.7974, -0.9760,
#           2.6178,  1.7238, -0.8376, -0.8023, -2.3062,  0.6210,  1.8459,  2.0546,
#           2.1673, -2.1704,  1.7475, -2.1072, -2.2381, -2.1801,  1.2248, -1.5315],
#         [ 1.2232,  0.5519,  1.1189, -0.3497,  0.5713,  1.2863,  0.8060, -1.3617,
#           1.0039, -0.6359, -0.2498, -0.6971, -0.9574,  0.7602,  0.4153, -0.7594,
#           0.0604, -0.1331,  2.0115, -0.9478, -0.6603, -0.0726,  0.3277,  0.6134,
#           0.9323, -1.3342,  0.7991, -0.1099, -1.0348, -0.2557,  0.3633, -1.4730],
#         [ 0.1211, -0.3762,  1.9552,  1.1474,  0.3533, -0.2194,  1.3023, -0.8038,
#          -0.3067,  0.3149,  0.6849, -0.1182, -0.1699,  1.1394, -0.6094, -0.6345,
#          -0.2910,  1.2123,  1.4075,  1.6931,  0.0889,  0.0997, -1.4151, -0.2416,
#           0.3129,  0.3485,  0.6185,  0.1375,  0.2082,  1.0440, -1.0737, -0.3461],
#         [ 0.0763,  0.6031, -0.5598,  0.3731, -1.5345,  1.9627,  1.1688, -0.6279,
#           2.1327,  0.1115,  1.1229,  0.7040,  0.2251, -0.8586, -0.8848,  0.5308,
#          -0.3943,  0.7656, -0.0551,  0.2902,  0.8235,  0.8392, -0.3259, -0.1366,
#           0.1226,  0.4747, -0.5956,  0.8132,  0.0358, -0.2269,  1.0477,  0.6342],
#         [-0.7307,  0.2059,  0.0029,  0.4834, -1.0785,  0.0384, -1.5464, -0.0266,
#          -0.3065,  0.6749,  0.4452, -0.2786,  0.6461, -0.4442, -0.3993, -0.8672,
#          -0.9674, -0.1972,  0.0734,  0.5813,  0.0988,  0.4089, -0.7483, -0.7719,
#          -0.8405,  0.8005, -0.0533,  0.5124,  1.1040,  0.0407, -0.0952,  0.0519],
#         [-1.1666,  1.9139, -1.6269,  0.9718,  0.5699, -0.9395, -0.9215,  0.4547,
#           0.3606,  1.2569, -1.2588,  1.3054,  0.1549, -0.9376, -1.9228,  1.5312,
#          -0.3425, -0.3005, -0.3914,  1.3572, -0.4680, -1.7157, -1.1395,  0.5300,
#          -0.3157, -0.1747,  0.6198, -0.7041,  0.4081,  0.1698,  1.4678,  1.2774],
#         [-0.7066, -1.0736, -0.0769, -1.2042, -0.6057, -0.1111, -1.3796,  0.4980,
#          -0.9298,  0.8423,  1.0279, -0.0382,  1.2981, -0.5712,  0.3522, -1.0179,
#          -0.8293, -1.6932, -1.3014, -0.0802,  0.4918,  1.2083,  0.4196, -1.3079,
#          -1.4372,  0.8138, -0.3185,  1.1282,  1.1757, -0.4797, -1.1796, -0.5190],
#         [-0.7511, -0.5560, -0.8840,  0.9830,  1.9583, -1.4241,  0.3183,  1.8698,
#          -1.3344,  0.0545, -0.6925,  1.1017,  0.4261, -0.9326,  0.6156,  1.4490,
#           0.3476, -0.7456, -0.4150, -0.7892,  0.6323, -1.7799, -0.1161,  0.4853,
#          -0.4178,  0.4526, -1.1731, -0.7788,  0.3269,  1.6495, -0.7741,  1.3006],
#         [-0.1847, -1.6873, -0.2550, -0.5186, -0.6043,  0.1167,  0.1700,  1.0365,
#           0.0689, -0.2359,  0.7456,  0.2450,  0.6681, -0.1757,  1.6359,  0.7440,
#          -0.2013, -0.6321, -0.4918, -1.3022,  1.2993,  0.3911,  1.1515, -1.2253,
#          -0.5238,  0.7891, -1.6445,  1.1086,  0.0142,  0.2385, -0.9810,  0.6055]],
#        grad_fn=<NativeBatchNormBackward0>)
# tensor([[-3.3114e-08, -2.6491e-08,  4.3048e-08,  7.2850e-08,  3.9736e-08,
#           2.6491e-08, -2.6491e-08,  0.0000e+00, -1.3245e-08, -1.3908e-07,
#          -9.2718e-08, -8.2784e-09,  0.0000e+00,  4.6359e-08,  0.0000e+00,
#           0.0000e+00,  6.6227e-09,  5.2982e-08,  2.6491e-08,  0.0000e+00,
#          -1.3245e-08,  1.9868e-08, -2.6491e-08, -1.3245e-08, -3.3114e-08,
#           1.5232e-07,  1.3245e-08,  1.3245e-08, -2.8561e-08,  6.6227e-09,
#           2.6491e-08,  6.6227e-09]], grad_fn=<UnsqueezeBackward0>)
# ++++++++++++++++++++++++++++++++++++++++
# tensor([1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<ReluBackward0>)
# tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
#        grad_fn=<ReluBackward0>)